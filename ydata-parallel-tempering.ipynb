{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1106 17:48:09.891010 4548449728 deprecation.py:506] From /Users/vpushkarov/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.summary.writer.writer import FileWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from models import build_simple_conv_net\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "N_REPLICAS = 6\n",
    "N_OUTPUTS = 4\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "N_GPUS = 3\n",
    "train_dict = {}\n",
    "\n",
    "def gpu_device_name(replica_id):\n",
    "    if N_GPUS:\n",
    "        return '/gpu:' + str(replica_id % N_GPUS)\n",
    "    return '/cpu:0'\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    inputs = tf.keras.layers.Input(shape=(28, 28, 1), dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope('targets'):\n",
    "    targets = tf.keras.layers.Input(shape=(), dtype=tf.int32)\n",
    "\n",
    "for i in range(N_REPLICAS):\n",
    "    dropout_rate = tf.compat.v1.placeholder_with_default(0., shape=())\n",
    "\n",
    "    train_dict[i] = {'dropout_rate': dropout_rate}\n",
    "    with tf.device(gpu_device_name(i)):\n",
    "        with tf.name_scope('model_' + str(i)):\n",
    "            train_dict[i]['model'] = build_simple_conv_net(inputs,\n",
    "                                                           dropout_rate,\n",
    "                                                           n_outputs=N_OUTPUTS)\n",
    "        with tf.name_scope('loss_' + str(i)):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=targets, logits=train_dict[i]['model'].outputs[0])\n",
    "            train_dict[i]['loss'] = tf.reduce_mean(xentropy)\n",
    "\n",
    "        with tf.name_scope('optimizer_' + str(i)):\n",
    "            grads = tf.gradients(train_dict[i]['loss'],\n",
    "                                 train_dict[i]['model'].trainable_variables)\n",
    "            grads_and_vars = zip(grads, train_dict[i]['model'].trainable_variables)\n",
    "\n",
    "            train_ops = [w.assign(w - LEARNING_RATE * g) for g, w in grads_and_vars]\n",
    "            train_dict[i]['train_op'] = tf.group(train_ops)\n",
    "\n",
    "        with tf.name_scope('error_' + str(i)):\n",
    "            y_pred = tf.argmax(train_dict[i]['model'].outputs[0], axis=1)\n",
    "            y_pred = tf.cast(y_pred, tf.int32)\n",
    "            y_true = tf.cast(targets, tf.int32)\n",
    "            equals = tf.cast(tf.math.equal(y_pred, y_true), tf.float32)\n",
    "            train_dict[i]['error'] = 1. - tf.reduce_mean(equals)\n",
    "\n",
    "\n",
    "FileWriter('logs/train', graph=inputs.graph).close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: 18565\n",
      "test data size: 4157\n",
      "validation data size 6189\n"
     ]
    }
   ],
   "source": [
    "dropout_list = np.linspace(0., 0.5, N_REPLICAS)\n",
    "for i, val in enumerate(dropout_list):\n",
    "    train_dict[i]['current_temperature'] = val\n",
    "\n",
    "    \n",
    "def create_empty_logs_dict():\n",
    "    logs = {'error_' + str(i): [] for i in range(N_REPLICAS)}\n",
    "    logs.update({'loss_' + str(i): [] for i in range(N_REPLICAS)})\n",
    "    logs.update({'temperature_' + str(i): [] for i in range(N_REPLICAS)})\n",
    "    logs['swap_success'] = 0\n",
    "    logs['swap_attempts'] = 0\n",
    "    return logs\n",
    "\n",
    "def append_to_log_dict(from_dict, to_dict):\n",
    "    \n",
    "    for metric, values in from_dict.items():\n",
    "        if not isinstance(values, list):\n",
    "            to_dict[metric] += values\n",
    "        elif 'temperature_' not in metric:\n",
    "            to_dict[metric].append(np.mean(values))\n",
    "        else:\n",
    "            to_dict[metric] += values\n",
    "    return to_dict\n",
    "    \n",
    "def iteritems(items, batch_size):\n",
    "    start_idx = 0\n",
    "    while start_idx < items.shape[0]:\n",
    "        yield items[start_idx: start_idx + batch_size]\n",
    "        start_idx += batch_size\n",
    "\n",
    "\n",
    "\n",
    "def dataset_splits():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train, x_test = x_train[..., None] / 255., x_test[..., None] / 255.\n",
    "    train_indices = np.argwhere(y_train < N_OUTPUTS).squeeze()\n",
    "    test_indices = np.argwhere(y_test < N_OUTPUTS).squeeze()\n",
    "    x_train, y_train = x_train[train_indices], y_train[train_indices]\n",
    "\n",
    "    x_test, y_test = x_test[test_indices], y_test[test_indices]\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train)\n",
    "    return (x_train, y_train), (x_test, y_test), (x_valid, y_valid)\n",
    "(x_train, y_train), (x_test, y_test), (x_valid, y_valid) = dataset_splits()\n",
    "\n",
    "datasets_dict = {\n",
    "    'x_train': x_train,\n",
    "    'y_train': y_train,\n",
    "    'x_test': x_test,\n",
    "    'y_test': y_test,\n",
    "    'x_valid': x_valid,\n",
    "    'y_valid': y_valid\n",
    "}\n",
    "\n",
    "print('train data size:', x_train.shape[0])\n",
    "print('test data size:', x_test.shape[0])\n",
    "print('validation data size', x_valid.shape[0])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_swap_replicas(train_dict, logs, coeff=1000.):\n",
    "    def _compute_losses():\n",
    "        data = datasets_dict['x_valid'], datasets_dict['y_valid']\n",
    "        logs = evaluate_on_epoch(train_dict, data)\n",
    "        losses = [logs['loss_' + str(i)] for i in range(N_REPLICAS)]\n",
    "        return losses\n",
    "    \n",
    "    losses = _compute_losses()\n",
    "    temperatures = [train_dict[i]['current_temperature'] for i in range(N_REPLICAS)]\n",
    "    \n",
    "    betas_ids_temperatures = [((1. - t) / t, review_id, t)\n",
    "                              for review_id, t in enumerate(temperatures)]\n",
    "    betas_ids_temperatures.sort(key=lambda x: x[0])\n",
    "    \n",
    "    picked_pair = np.random.randint(low=0, high=N_REPLICAS - 1)\n",
    "\n",
    "    replica_i = betas_ids_temperatures[picked_pair][1]\n",
    "    replica_j = betas_ids_temperatures[picked_pair + 1][1]\n",
    "\n",
    "    temperature_i = betas_ids_temperatures[picked_pair][2]\n",
    "    temperature_j = betas_ids_temperatures[picked_pair + 1][2]\n",
    "    \n",
    "    beta_i = betas_ids_temperatures[picked_pair][0]\n",
    "    beta_j = betas_ids_temperatures[picked_pair + 1][0]\n",
    "    \n",
    "    loss_i = losses[replica_i], \n",
    "    loss_j = losses[replica_j]\n",
    "\n",
    "    proba = np.exp(coeff * (loss_i - loss_j) * (beta_i - beta_j))\n",
    "    if np.random.uniform() < proba:\n",
    "        swap_success = 1\n",
    "        train_dict[replica_i]['current_temperature'] = temperature_j\n",
    "        train_dict[replica_j]['current_temperature'] = temperature_i\n",
    "    else:\n",
    "        swap_success = 0\n",
    "    logs['swap_success'] += swap_success\n",
    "    logs['swap_attempts'] += 1\n",
    "    logs['validation']\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(train_dict, data):\n",
    "\n",
    "    x_data, y_data = data\n",
    "    feed_dict = {inputs: x_data,\n",
    "                 targets: y_data}\n",
    "    feed_dict.update({train_dict[i]['dropout_rate']: train_dict[i]['current_temperature']\n",
    "                      for i in range(N_REPLICAS)})\n",
    "    train_ops = [train_dict[i]['train_op'] for i in range(N_REPLICAS)]\n",
    "    losses = [train_dict[i]['loss'] for i in range(N_REPLICAS)]\n",
    "    errors = [train_dict[i]['error'] for i in range(N_REPLICAS)]\n",
    "    sess = tf.compat.v1.get_default_session()\n",
    "    evaled = sess.run(losses + errors + train_ops, feed_dict=feed_dict)\n",
    "    losses = evaled[:N_REPLICAS]\n",
    "    errors = evaled[N_REPLICAS: 2 * N_REPLICAS]\n",
    "\n",
    "    return losses, errors\n",
    "\n",
    "def evaluate_on_batch(train_dict, data):\n",
    "    x_data, y_data = data\n",
    "    feed_dict = {inputs: x_data,\n",
    "                 targets: y_data}\n",
    "    feed_dict.update({train_dict[i]['dropout_rate']: train_dict[i]['current_temperature']\n",
    "                      for i in range(N_REPLICAS)})\n",
    "    losses = [train_dict[i]['loss'] for i in range(N_REPLICAS)]\n",
    "    errors = [train_dict[i]['error'] for i in range(N_REPLICAS)]\n",
    "    sess = tf.compat.v1.get_default_session()\n",
    "    evaled = sess.run(losses + errors, feed_dict=feed_dict)\n",
    "    losses = evaled[:N_REPLICAS]\n",
    "    errors = evaled[N_REPLICAS: 2 * N_REPLICAS]\n",
    "    return losses, errors\n",
    "\n",
    "def train_on_epoch(train_dict, data, step, swap_step):\n",
    "    (x_train, y_train) = datasets_dict['x_train'], datasets_dict['y_train']\n",
    "    \n",
    "    zipped_batches = zip(iteritems(x_train, BATCH_SIZE),\n",
    "                         iteritems(y_train, BATCH_SIZE))\n",
    "    logs = create_empty_logs_dict()\n",
    "\n",
    "    for (x_batch, y_batch) in zipped_batches:\n",
    "        losses, errors = train_on_batch(train_dict,\n",
    "                                        (x_batch, y_batch))\n",
    "\n",
    "        for i in range(N_REPLICAS):\n",
    "            logs['error_' + str(i)].append(errors[i])\n",
    "        for i in range(N_REPLICAS):\n",
    "            logs['loss_' + str(i)].append(losses[i])\n",
    "        for i in range(N_REPLICAS):\n",
    "            logs['temperature_' + str(i)].append(train_dict[i]['current_temperature'])\n",
    "        \n",
    "        step += 1\n",
    "        if step % swap_step == 0:\n",
    "            logs = maybe_swap_replicas(train_dict, logs)\n",
    "    return logs, step\n",
    "\n",
    "def evaluate_on_epoch(train_dict, data):\n",
    "    (x_train, y_train) = data\n",
    "    \n",
    "    zipped_batches = zip(iteritems(x_train, BATCH_SIZE),\n",
    "                         iteritems(y_train, BATCH_SIZE))\n",
    "    logs = create_empty_logs_dict()\n",
    "    for (x_batch, y_batch) in zipped_batches:\n",
    "        losses, errors = train_on_batch(train_dict,\n",
    "                                        (x_batch, y_batch))\n",
    "        for i in range(N_REPLICAS):\n",
    "            logs['error_' + str(i)].append(errors[i])\n",
    "        for i in range(N_REPLICAS):\n",
    "            logs['loss_' + str(i)].append(losses[i])\n",
    "\n",
    "    for metric, values in logs.items():\n",
    "        if isinstance(values, list):\n",
    "            logs[metric] = np.mean(values)\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vpushkarov/anaconda3/envs/tf14/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/vpushkarov/anaconda3/envs/tf14/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/vpushkarov/anaconda3/envs/tf14/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c1bf19933ba4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m logs = train(train_dict,\n\u001b[1;32m     38\u001b[0m              \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m              swap_step=100)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-c1bf19933ba4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dict, data, swap_step)\u001b[0m\n\u001b[1;32m     19\u001b[0m                                         \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                         \u001b[0mcurrent_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                         swap_step)\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mall_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappend_to_log_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-84fcc357c769>\u001b[0m in \u001b[0;36mtrain_on_epoch\u001b[0;34m(train_dict, data, step, swap_step)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mswap_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_swap_replicas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6e0982b87f0f>\u001b[0m in \u001b[0;36mmaybe_swap_replicas\u001b[0;34m(train_dict, logs, coeff)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'swap_success'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mswap_success\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'swap_attempts'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation'"
     ]
    }
   ],
   "source": [
    "def train(train_dict,\n",
    "          data,\n",
    "          swap_step=100):\n",
    "    (x_train, y_train, x_test, y_test, x_valid, y_valid) = data\n",
    "\n",
    "    current_step = 0\n",
    "    all_logs = {\n",
    "        'train': create_empty_logs_dict(),\n",
    "        'test': create_empty_logs_dict(),\n",
    "        'validation': create_empty_logs_dict()\n",
    "    }\n",
    "\n",
    "    graph = train_dict[0]['model'].inputs[0].graph\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    with tf.Session(graph=graph, config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(EPOCHS):\n",
    "            logs, step = train_on_epoch(train_dict,\n",
    "                                        (x_train, y_train),\n",
    "                                        current_step,\n",
    "                                        swap_step)\n",
    "            all_logs['train'] = append_to_log_dict(logs, all_logs['train'])\n",
    "\n",
    "            logs = evaluate_on_epoch(train_dict,\n",
    "                                     (x_test, y_test))\n",
    "            all_logs['test'] = append_to_log_dict(logs, all_logs['test'])\n",
    "\n",
    "            logs = evaluate_on_epoch(train_dict,\n",
    "                                     (x_valid, y_valid))\n",
    "            all_logs['validation'] = append_to_log_dict(logs, all_logs['validation'])\n",
    "\n",
    "            current_step += step\n",
    "    return all_logs\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_valid, y_valid) = dataset_splits()\n",
    "\n",
    "logs = train(train_dict,\n",
    "             (x_train, y_train, x_test, y_test, x_valid, y_valid),\n",
    "             swap_step=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
