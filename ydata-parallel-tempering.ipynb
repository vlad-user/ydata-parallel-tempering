{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1102 16:02:53.007150 4450002368 deprecation.py:506] From /Users/vpushkarov/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.summary.writer.writer import FileWriter\n",
    "import numpy as np\n",
    "\n",
    "from models import build_model\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "n_replicas = 4\n",
    "train_dict = {}\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    inputs = tf.keras.layers.Input(shape=(28, 28, 1), dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope('targets'):\n",
    "    targets = tf.keras.layers.Input(shape=(), dtype=tf.int32)\n",
    "\n",
    "for i in range(n_replicas):\n",
    "    dropout_rate = tf.compat.v1.placeholder_with_default(0., shape=())\n",
    "    lr = tf.compat.v1.placeholder(tf.float32, shape=())\n",
    "\n",
    "    train_dict[i] = {'lr': lr,\n",
    "                     'dropout_rate': dropout_rate}\n",
    "\n",
    "    with tf.name_scope('model_' + str(i)):\n",
    "        train_dict[i]['model'] = build_model(inputs, dropout_rate)\n",
    "    \n",
    "    with tf.name_scope('loss_' + str(i)):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=targets, logits=train_dict[i]['model'].outputs[0])\n",
    "        train_dict[i]['loss'] = tf.reduce_mean(xentropy)\n",
    "    \n",
    "    with tf.name_scope('optimizer_' + str(i)):\n",
    "        grads = tf.gradients(train_dict[i]['loss'],\n",
    "                             train_dict[i]['model'].trainable_variables)\n",
    "        grads_and_vars = zip(grads, train_dict[i]['model'].trainable_variables)\n",
    "        with tf.compat.v1.control_dependencies(grads):\n",
    "            train_ops = [w.assign(w - lr * g) for g, w in grads_and_vars]\n",
    "            train_dict[i]['train_op'] = tf.group(train_ops)\n",
    "    \n",
    "    with tf.name_scope('error_' + str(i)):\n",
    "        y_pred = tf.argmax(train_dict[i]['model'].outputs[0], axis=1)\n",
    "        equals = tf.cast(tf.math.equal(y_pred, y_true), tf.float32)\n",
    "        train_dict[i]['error'] = 1. - tf.reduce_mean(equals)\n",
    "FileWriter('logs/train', graph=inputs.graph).close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = np.linspace(0.01, 0.001, 4)\n",
    "for i, lr_val in enumerate(lr_list):\n",
    "    train_dict[i]['noise_val'] = lr_val\n",
    "\n",
    "def train_on_batch(train_dict, data, to_swap='lr'):\n",
    "    x_data, y_data = data\n",
    "    feed_dict = {inputs: x_data,\n",
    "                 targets: y_data}\n",
    "    feed_dict.update({train_dict[i][to_swap]: train_dict[i]['noise_val']})\n",
    "    train_ops = [train_dict[i]['train_op'] for i in range(n_replicas)]\n",
    "    losses = [train_dict[i]['loss'] for i in range(n_replicas)]\n",
    "    errors = [train_dict[i]['error'] for i in range(n_replicas)]\n",
    "    evaled = sess.run(losses + errors + train_ops, feed_dict=feed_dict)\n",
    "    losses = evaled[:n_replicas]\n",
    "    errors = evaled[n_replicas: 2 * n_replicas]\n",
    "    return losses, errors\n",
    "\n",
    "def swap_replicas(train_dict, losses, logs, coeff=1., to_swap='lr'):\n",
    "\n",
    "    temperatures = [train_dict[i]['noise_val'] for i in range(n_replicas)]\n",
    "    if 'lr' == to_swap:\n",
    "        betas_and_ids = [(1. / b, i, b) for i, b in enumerate(temperatures)]\n",
    "    else:\n",
    "        betas_and_ids = [(t / (1. - t), i, t) for i, b in enumerate(temperatures)]\n",
    "    betas_and_ids.sort(key=lambda x: x[0])\n",
    "    \n",
    "    random_pair = np.random.randint(low=0, high=n_replicas - 1)\n",
    "    i = betas_and_ids[random_pair][1]\n",
    "    j = betas_and_ids[random_pair + 1][1]\n",
    "    temp_i = betas_and_ids[random_pair][2]\n",
    "    beta_i = betas_and_ids[random_pair][0]\n",
    "    temp_j = betas_and_ids[random_pair + 1][2]\n",
    "    beta_j = betas_and_ids[random_pair + 1][0]\n",
    "    loss_i, loss_j = losses[i], losses[j]\n",
    "    \n",
    "    proba = np.exp(coeff * (loss_i - loss_j) * (beta_i - beta_j))\n",
    "    if np.random.uniform() < proba:\n",
    "        swap_success = 1\n",
    "        train_dict[i] = temp_j\n",
    "        train_dict[j] = temp_i\n",
    "    else:\n",
    "        swap_success = 0\n",
    "    logs['swap_success'] += swap_success\n",
    "    logs['swap_attempts'] += 1\n",
    "    \n",
    "\n",
    "\n",
    "def train(train_dict,\n",
    "          data,\n",
    "          batch_size=32,\n",
    "          epochs=32,\n",
    "          swap_step=100,\n",
    "          to_swap='lr'):\n",
    "    (x_train, y_train, x_test, y_test, x_valid, y_valid) = data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
